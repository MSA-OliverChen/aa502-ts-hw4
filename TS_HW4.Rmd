---
title: "Time Series Homework 4"
author: "Team 6 - Oliver Chen, Hannah Enck, Jamil Gracia, Ava Klissouras"
date: "2025-10-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

Load libraries.

```{r}
library(tsibble)
library(tidyverse)
library(ggplot2)
library(forecast)
library(fable)
library(fpp3)
library(tseries)
```


Read in data.

```{r}
data_path = "https://raw.githubusercontent.com/sjsimmo2/TimeSeries/refs/heads/master/energy_F2025.csv"
energy <- readr::read_csv(data_path, show_col_types=FALSE)
```

Roll up data.

```{r}
energy_daily <- energy %>%
  mutate(new_datetime = mdy_hm(datetime_beginning_ept),
         date = as_date(new_datetime)) %>%
  group_by(date) %>%
  summarise(daily_total = sum(mw))
```


Train test split.

```{r}
#Train: data from August 2018 - August 2023
train <- energy_daily %>%
  filter(date >= as.Date("2018-08-01") & date < as.Date("2023-08-01"))

train.ts = train %>%
  as_tsibble(index = date)

#Validation: data from September 2023 - August 2024
validation <- energy_daily %>%
  filter(date >= as.Date("2023-08-01") & date < as.Date("2024-08-01"))

validation.ts = validation %>%
  as_tsibble(index = date)

#Test: data from September 2024 - August 2025
test <- energy_daily %>%
  filter(date >= as.Date("2024-08-01") & date < as.Date("2025-08-01"))

test.ts = test %>%
  as_tsibble(index = date)
```


Check for missing values.

```{r}
has_gaps(train.ts)
has_gaps(validation.ts)
has_gaps(test.ts)

#All return FALSE- good. No implicit missingness
```

# Analysis

## Exponential Smoothing Model (ESM)

### Performing STL decomposition on training data to assess trend/seasonality

```{r}
dcmp = train.ts %>% model (stl = STL (daily_total))
components(dcmp) %>% autoplot() + theme_classic()
```

```{r}
train.ts %>% 
  features(daily_total, feat_stl, .period = "year")

train.ts %>% 
  features(daily_total, feat_stl, .period = 7)

#Yearly seasonal strength is much stronger than weekly, so we will just stick with year

```


### Fitting models
```{r}
esm_fit <- train.ts |>
  model(
    #Just weekly seasonality since ESM does not support periods longer than 24 (i.e. year would be 365 since we have daily data)
    'HWAddWeek' = ETS(daily_total ~ error("A") + trend("A") + season("A", period="week")),
    'HWMultWeek' = ETS(daily_total ~ error("M") + trend("A") + season("M", period="week")),
    'NoTrendWeek' = ETS(daily_total ~ error("A") + trend("N") + season("A", period="week")),
  )
```

```{r}
esm_fc <- esm_fit %>%
  fabletools::forecast(h = nrow(validation))
fabletools::accuracy(esm_fc, validation.ts)
```

```{r}
#Search for best one
esm_auto <- train.ts %>%
  model(ETS(daily_total))

report(esm_auto) #MNA

auto_fc <- esm_auto %>%
  fabletools::forecast(h = nrow(validation)-1)
fabletools::accuracy(auto_fc, validation.ts)

#Worse than the ones we did manually

```

### Actual vs. Forecasted Plot

```{r}
#Best model of all the ones above was Holt-Winters additive

#Get forecast
HW_add <- train.ts |>
  model(
    ETS(daily_total ~ error("A") + trend("A") + season("A", period="week"))
  )
ESM_fc <- HW_add %>%
  fabletools::forecast(h = nrow(validation)-1)

ggplot() + geom_line(data = ESM_fc, aes(x=date, y=.mean, color = "Forecast")) + 
  geom_line(data = validation.ts, aes(x=date, y=daily_total, color = "Actual"))+
  labs(x="Month", y="Megawatts", title="Total Daily Energy Consumption", color = "Series") + 
  scale_color_manual (values=c("Actual"="black","Forecast" = "orange" ))
```

## Seasonal ARIMA

Recall that yearly seasonality was the strongest, but weekly seasonality is still present. With multiple seasonalities and yearly being extremely complex, fitting Fourier terms is the most realistic option.

```{r}
train.ts %>%
  features(daily_total, feat_stl, .period = 'week')

train.ts %>%
  features(daily_total, feat_stl, .period = 'year')
```

### Approaches

#### Deterministic

##### Dummy Variables

```{r}
# weekly seasonality only realistic one to do
sd_arima <- train.ts %>%
  mutate(day_of_week = wday(date)) %>%
  model(ARIMA(daily_total ~ factor(day_of_week) + PDQ(D = 0) + pdq(d = 0)))
report(sd_arima)
```

```{r}
sd_arima <- train.ts %>%
  mutate(day_of_year = yday(date)) %>%
  model(ARIMA(daily_total ~ factor(day_of_year) + PDQ(D = 0) + pdq(d = 0)))
report(sd_arima)
```


#### Fourier

Let's try to do a grid search for both weekly and yearly seasonality. Note here that we use $P=Q=D=d=0$ because we want to try to capture non-stationarity through the seasonal components only.

```{r}
grid_fourier = function(train,
                        K_year_max = 12, 
                        K_week_max = 3) {
  for (K_year in c(1:K_year_max)) {
    for (K_week in c(1:K_week_max)) {
      iter_name = paste0("K_year=", K_year, ", K_week=", K_week, sep='')
      print(iter_name)
      model_iter_glance = train %>%
        model(
          # jank way to set the name based on current iteration
          !!iter_name :=
            ARIMA(daily_total ~ fourier(K=K_week, period = 'week') + 
                                fourier(K=K_year, period = 'year') + 
                                PDQ(0, 0, 0) + 
                                pdq(d = 0)
            )
        ) %>%
        glance()
      # use first one to initialize master
      if (K_year == 1 & K_week == 1) {
        fourier_master = model_iter_glance
      } else {
        fourier_master = bind_rows(fourier_master, model_iter_glance)
      }
    }
  }
  return (fourier_master)
}

# set K_year_max to determine the upper limit for the yearly search
# K_year_max = 12 is sufficient to get the top 10 best models
# K_year_max = 25 is an arbitrary upper limit that has similar results but is
#                 ultimately more exhaustive
grid_fourier_results = grid_fourier(train.ts)
grid_fourier_results %>%
  arrange(AICc)
```

This one found a very similar model, with the non-grid search being a narrow second in AICc. Let's use the final model from the grid search. 

```{r}
K_year = 6
K_week = 3

fourier_final = train.ts %>%
  model(
    ARIMA(daily_total ~ fourier(K=K_week, period = 'week') + 
                        fourier(K=K_year, period = 'year') + 
                        PDQ(0, 0, 0) + pdq(0, 0, 0))
  )

report(fourier_final)
```

Let's check if we have trending residuals by checking for trend stationarity in the residuals of `fourier_final`, which are essentially the seasonally adjusted data.

```{r}
# here, test for stationarity around a deterministic trend
# if failing to reject H_0, data is stationary, so conclude a deterministic trend
# if rejecting H_0, data is non-stationary, so conclude a stochastic trend
fourier_final %>%
  resid() %>%
  fabletools::features(.resid, unitroot_kpss, .null = 'Trend')
```

The KPSS test rejected the null hypothesis at a significance level of 0.05, so we choose to model with stochastic trend.

```{r}
final_arima = train.ts %>%
  model(
    ARIMA(daily_total ~ fourier(K=K_week, period = 'week') + 
                        fourier(K=K_year, period = 'year') +
                        PDQ(0, 0, 0) + 
                        pdq(d=1)
    )
  )

report(final_arima)
```

Let's double-check that we have white noise.

```{r}
# this should be min(2 * longest seasonality, n / 5), according to Hyndman
lag = 2 * 365.25

arima_dof = function(model) {
  mod = model[[1]][[1]]
  p = mod$fit$spec[[1]]
  q = mod$fit$spec[[3]]
  return(p+q)
}
dof = arima_dof(final_arima)

# Ljung-Box test failed to reject H_0, so we conclude no significant autocorrelation
augment(final_arima) %>%
  features(.innov, ljung_box, lag=lag, dof=dof)

# Homoscedasticity: residuals do not appear to vary over time
# Autocorrelation: the ACF plot shows just one slight spike for lags > 4, so 
#                  I feel comfortable concluding no significant autocorrelation
# Normality: the distribution appears roughly symmetric
final_arima %>%
  ggtime::gg_tsresiduals()

# Normality: the tails are somewhat heavy but the residuals are mostly normal
final_arima %>%
  augment() %>%
  ggplot(aes(sample = .resid)) +
    stat_qq() +
    stat_qq_line() +
    labs(title = "Q-Q Plot")
```


### Forecast on Validation

```{r}
arima_fc <- final_arima %>%
  fabletools::forecast(h = nrow(validation.ts))

ggplot() + 
  geom_line(data = arima_fc , aes(x=date, y=.mean, color = "Forecast")) + 
  geom_line(data = validation.ts, aes(x=date, y=daily_total, color = "Actual")) +
  labs(x="Month", y="Megawatts", title="Total Daily Energy Consumption", color = "Series") + 
  scale_color_manual (values=c("Actual"="black","Forecast" = "orange" ))
```


### Model Accuracy

```{r}
# get SD for MAE report
energy_daily %>% pull(daily_total) %>% sd()

fabletools::accuracy(arima_fc, validation.ts) %>%
  select(c(MAE, MAPE))
```

Accuracy measures of the model are as follows:
* MAE: 7299.94 (SD = 15300.61)
* MAPE: 6.77%
