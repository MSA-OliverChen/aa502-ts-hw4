---
title: "Time Series Homework 4"
author: "Team 6 - Oliver Chen, Hannah Enck, Jamil Gracia, Ava Klissouras"
date: "2025-10-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

Load libraries.

```{r}
library(tsibble)
library(tidyverse)
library(ggplot2)
library(forecast)
library(fable)
library(fpp3)
library(tseries)
library(progressr)

progressr::handlers(handler_cli)
```


Read in data.

```{r}
data_path = "https://raw.githubusercontent.com/sjsimmo2/TimeSeries/refs/heads/master/energy_F2025.csv"
energy <- readr::read_csv(data_path, show_col_types=FALSE)
```

Roll up data.

```{r}
energy_daily <- energy %>%
  mutate(new_datetime = mdy_hm(datetime_beginning_ept),
         date = as_date(new_datetime)) %>%
  group_by(date) %>%
  summarise(daily_total = sum(mw))
```


Train test split.

```{r}
#Train: data from August 2018 - August 2023
train <- energy_daily %>%
  filter(date >= as.Date("2018-08-01") & date < as.Date("2023-08-01"))

train.ts = train %>%
  as_tsibble(index = date)

#Validation: data from September 2023 - August 2024
validation <- energy_daily %>%
  filter(date >= as.Date("2023-08-01") & date < as.Date("2024-08-01"))

validation.ts = validation %>%
  as_tsibble(index = date)

#Test: data from September 2024 - August 2025
test <- energy_daily %>%
  filter(date >= as.Date("2024-08-01") & date < as.Date("2025-08-01"))

test.ts = test %>%
  as_tsibble(index = date)
```


Check for missing values.

```{r}
has_gaps(train.ts)
has_gaps(validation.ts)
has_gaps(test.ts)

#All return FALSE- good. No implicit missingness
```

# Analysis

## Exponential Smoothing Model (ESM)

### Performing STL decomposition on training data to assess trend/seasonality

```{r}
dcmp = train.ts %>% model (stl = STL (daily_total))
components(dcmp) %>% autoplot() + theme_classic()
```

```{r}
train.ts %>% 
  features(daily_total, feat_stl, .period = "year")

train.ts %>% 
  features(daily_total, feat_stl, .period = 7)

#Yearly seasonal strength is much stronger than weekly, so we will just stick with year

```


### Fitting models
```{r}
esm_fit <- train.ts |>
  model(
    #Just weekly seasonality since ESM does not support periods longer than 24 (i.e. year would be 365 since we have daily data)
    'HWAddWeek' = ETS(daily_total ~ error("A") + trend("A") + season("A", period="week")),
    'HWMultWeek' = ETS(daily_total ~ error("M") + trend("A") + season("M", period="week")),
    'NoTrendWeek' = ETS(daily_total ~ error("A") + trend("N") + season("A", period="week")),
  )
```

```{r}
esm_fc <- esm_fit %>%
  fabletools::forecast(h = nrow(validation))
fabletools::accuracy(esm_fc, validation.ts)
```

```{r}
#Search for best one
esm_auto <- train.ts %>%
  model(ETS(daily_total))

report(esm_auto) #MNA

auto_fc <- esm_auto %>%
  fabletools::forecast(h = nrow(validation)-1)
fabletools::accuracy(auto_fc, validation.ts)

#Worse than the ones we did manually

```

### Actual vs. Forecasted Plot

```{r}
#Best model of all the ones above was Holt-Winters additive

#Get forecast
HW_add <- train.ts |>
  model(
    ETS(daily_total ~ error("A") + trend("A") + season("A", period="week"))
  )
ESM_fc <- HW_add %>%
  fabletools::forecast(h = nrow(validation)-1)

ggplot() + geom_line(data = ESM_fc, aes(x=date, y=.mean, color = "Forecast")) + 
  geom_line(data = validation.ts, aes(x=date, y=daily_total, color = "Actual"))+
  labs(x="Month", y="Megawatts", title="Total Daily Energy Consumption", color = "Series") + 
  scale_color_manual (values=c("Actual"="black","Forecast" = "orange" ))
```

## Seasonal ARIMA

Recall that yearly seasonality was the strongest, but weekly seasonality is still present. With multiple seasonalities and yearly being extremely complex, fitting Fourier terms is the most realistic option.

```{r}
train.ts %>%
  features(daily_total, feat_stl, .period = 'week')

train.ts %>%
  features(daily_total, feat_stl, .period = 'year')
```

### Approaches

#### Deterministic

##### Dummy Variables

```{r}
# weekly seasonality only realistic one to do
sd_arima <- train.ts %>%
  mutate(day_of_week = wday(date)) %>%
  model(ARIMA(daily_total ~ factor(day_of_week) + PDQ(D = 0) + pdq(d = 0)))
report(sd_arima)
```

```{r}
# don't run this lol
# sd_arima <- train.ts %>%
#   mutate(day_of_year = yday(date)) %>%
#   model(ARIMA(daily_total ~ factor(day_of_year) + PDQ(D = 0) + pdq(d = 0)))
# report(sd_arima)
```


#### Fourier

Let's try to do a grid search for both weekly and yearly seasonality. Note here that we use $P=Q=D=d=0$ because we want to try to capture non-stationarity through the seasonal components only.

```{r}
grid_fourier = function(train,
                        K_year_min = 1,
                        K_year_max = 12, 
                        K_week_min = 1,
                        K_week_max = 3) {
  # ensure Ks are within bounds
  K_year_min = max(1, K_year_min)
  K_week_min = max(1, K_week_min)
  K_year_max = min(182, K_year_max)
  K_week_max = min(3, K_week_max) 
  
  fourier_master = list()
  for (K_year in c(K_year_min:K_year_max)) {
    for (K_week in c(K_week_min:K_week_max)) {
      iter_name = paste0("K_year=", K_year, ", K_week=", K_week, sep='')
      # create the model object for this iteration
      arima = (
        # !! ensures that we fill in the current values of K_week and K_year
        # without it, these model definitions all get evaluated at the end of
        #   iteration and just use the largest values
        ARIMA(daily_total ~ fourier(K=!!K_week, period = 'week') + 
                            fourier(K=!!K_year, period = 'year') + 
                            PDQ(0, 0, 0) + 
                            pdq(0, 0, 0))
      )
      
      # insert the model into the list of models
      fourier_master[[iter_name]] = arima
    }
  }

  progressr::with_progress({
    out = train %>%
      model(!!!fourier_master)
  })
  
  return (out)
}

# set K_year_max to determine the upper limit for the yearly search
grid_fourier_results = grid_fourier(train.ts, K_year_max = 25, K_week_min = 3)
grid_fourier_results %>%
  glance() %>%
  arrange(BIC) %>%
  filter(str_detect(.model, 'K_week=3'))
```

While $K_week=2$ models give improved BIC, it turns out that they are not adequately capturing weekly seasonality. We will thus choose to use the best $K_week=3$ model.

```{r}
K_year = 11
K_week = 3
```

Let's check if we have trending residuals by checking for trend stationarity in the residuals of `fourier_final`, which are essentially the seasonally adjusted data.

```{r}
fourier_final = train.ts %>%
  model(
    ARIMA(daily_total ~ fourier(K=K_week, period = 'week') + 
                        fourier(K=K_year, period = 'year') + 
                        PDQ(0, 0, 0) + pdq(0, 0, 0))
  )

# here, test for stationarity around a deterministic trend
# if failing to reject H_0, data is stationary, so conclude a deterministic trend
# if rejecting H_0, data is non-stationary, so conclude a stochastic trend
fourier_final %>%
  resid() %>%
  fabletools::features(.resid, unitroot_kpss, .null = 'trend')

fourier_final %>%
  resid() %>%
  fabletools::features(.resid, unitroot_ndiffs)
```

The KPSS test rejected the null hypothesis at a significance level of 0.05, so we choose to model with stochastic trend.

```{r}
search_arima = train.ts %>%
  model(
    ARIMA(daily_total ~ fourier(K=K_week, period = 'week') + 
                        fourier(K=K_year, period = 'year') +
                        PDQ(0, 0, 0) + 
                        pdq(d=1)
    )
  )

report(search_arima)
glance(search_arima)

search_arima %>%
  resid() %>%
  ggAcf()

search_arima %>%
  resid() %>%
  ggPacf()
```

```{r}
manual_arima = train.ts %>%
  model(
    ARIMA(daily_total ~ fourier(K=K_week, period = 'week') + 
                        fourier(K=K_year, period = 'year') +
                        PDQ(0, 0, 0) + 
                        pdq(1, 1, 3) +
                        0
    )
  )

report(manual_arima)
glance(manual_arima)

manual_arima %>%
  resid() %>%
  ggAcf()

manual_arima %>%
  resid() %>%
  ggPacf()
```


Let's double-check that we have white noise.

```{r}
# final model choice for evaluation
final_arima = search_arima

# this should be min(2 * longest seasonality, n / 5), according to Hyndman
# lag = 2 * 365.25
# also trying 3 * shortest seasonality
lag = 3 * 7

arima_dof = function(model) {
  mod = model[[1]][[1]]
  p = mod$fit$spec[[1]]
  q = mod$fit$spec[[3]]
  return(p+q)
}
dof = arima_dof(final_arima)

# Ljung-Box test failed to reject H_0, so we conclude no significant autocorrelation
augment(final_arima) %>%
  features(.innov, ljung_box, lag=lag, dof=dof)

# Homoscedasticity: residuals do not appear to vary over time
# Autocorrelation: the ACF plot shows just one slight spike for lags > 4, so 
#                  I feel comfortable concluding no significant autocorrelation
# Normality: the distribution appears roughly symmetric
final_arima %>%
  ggtime::gg_tsresiduals()

# Normality: the tails are somewhat heavy but the residuals are mostly normal
final_arima %>%
  augment() %>%
  ggplot(aes(sample = .resid)) +
    stat_qq() +
    stat_qq_line() +
    labs(title = "Q-Q Plot")
```


### Forecast on Validation

```{r}
arima_fc <- final_arima %>%
  fabletools::forecast(h = nrow(validation.ts))

ggplot() + 
  geom_line(data = arima_fc , aes(x=date, y=.mean, color = "Forecast")) + 
  geom_line(data = validation.ts, aes(x=date, y=daily_total, color = "Actual")) +
  labs(x="Date", y="Megawatts", title="Total Daily Energy Consumption", color = "Series") + 
  scale_color_manual (values=c("Actual"="black","Forecast" = "orange" ))
```


### Model Accuracy

```{r}
# get SD for MAE report
energy_daily %>% pull(daily_total) %>% mean()

fabletools::accuracy(arima_fc, validation.ts) %>%
  select(c(MAE, MAPE))
```

* ARIMA(0, 1, 4) + (K_year = 6, K_week = 3)
  * MAE: 7299.94 (mean = 105087.8)
  * MAPE: 6.77%
* ARIMA(1, 1, 3) + (K_year = 6, K_week = 3)
  * MAE: 7247.61 (mean = 105087.8)
  * MAPE: 6.72%
* ARIMA(0, 1, 4) + (K_year = 11, K_week = 3)
  * MAE: 7169.08 (mean = 105087.8)
  * MAPE: 6.66%
* ARIMA(1, 1, 3) + (K_year = 11, K_week = 3)
  * MAE: 7147.81 (mean = 105087.8)
  * MAPE: 6.64%

